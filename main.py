# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jzEbwMO9xKaT0XOmS2mmk9TxsKlbQav_
"""

!pip install -q notebook langchain langchain-google-genai sentence-transformers faiss-cpu pypdf unstructured python-dotenv

!pip install langchain-community

import os
from dotenv import load_dotenv
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader, UnstructuredPowerPointLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

PATH = "/content/drive/MyDrive/waffles/"
DOCUMENTS_PATH = os.path.join(PATH, "data/BME")
DB_FAISS_PATH = os.path.join(PATH, 'db')

!pip install "unstructured[all-docs]"
!pip install python-pptx

ppt_loader = DirectoryLoader(
    DOCUMENTS_PATH,
    glob="**/*.pptx",
    loader_cls=UnstructuredPowerPointLoader,
    use_multithreading=True,
    show_progress=True
)

pdf_loader = DirectoryLoader(
    DOCUMENTS_PATH,
    glob="**/*.pdf",
    loader_cls=PyPDFLoader,
    use_multithreading=True,
    show_progress=True
)

ppts = ppt_loader.load()
pdfs = pdf_loader.load()

documents = ppts + pdfs

print(f"Loaded {len(documents)} documents.")

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
texts = text_splitter.split_documents(documents)
print(f"Split into {len(texts)} text chunks.")

embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})

db = FAISS.from_documents(texts, embeddings)
db.save_local(DB_FAISS_PATH)

print(f"Vector database created and saved to {DB_FAISS_PATH}")

from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA

from google.colab import userdata
GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')

prompt_template = """Use the following pieces of context to answer the question at the end. Elaborate the answer so that it is easy to understand.
Context: {context}
Question: {question}
Answer:
"""

prompt = PromptTemplate(template = prompt_template, input_variables=["context", "question"])

llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=GOOGLE_API_KEY)

from langchain.chains import LLMChain

non_rag_chain = LLMChain(
    llm=llm,
    prompt=PromptTemplate.from_template("{question}")
)

db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type='stuff',
    retriever=db.as_retriever(search_kwargs={'k': 2}),
    return_source_documents=True,
    chain_type_kwargs={'prompt': prompt}
)

print("Type 'exit' or 'quit' to stop.")

while True:
    query = input("> ")
    if query.lower() in ['exit', 'quit']:
        break

    try:
        res_rag = qa_chain.invoke({'query': query})
        answer_rag = res_rag["result"]
        source_docs = res_rag["source_documents"]

        print(answer_rag)
        if source_docs:
            print("\nSources:")
            for doc in source_docs:
                print(f"- {os.path.basename(doc.metadata.get('source', 'Unknown'))}")

    except Exception as e:
        print(f"\nError details: {e}")

